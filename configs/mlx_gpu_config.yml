# MLX GPU Configuration for Apple Silicon M2 Max

## Overview

This configuration optimizes MLX (Apple Machine Learning for X) for the M2 Max chip with 64GB RAM and 50GB VRAM.

## Hardware Specifications
## Mole Status  Health ● 100  MacBook Pro · Apple M2 Max (30GPU) · 64.0 GB/1.8 TB · macOS 26.2
**Chip**: Apple M2 Max
**RAM**: 64GB
**VRAM**: 50GB (available for MLX)
**CPU Cores**: 12
**GPU Cores**: Up to 5
**Memory Bandwidth**: 800 GB/s

## GPU Configuration

### Tensor Parallelism

**tensor_parallel**: 5
**max_gpus**: 5

**Rationale**: Distributing computation across 5 GPUs for maximum throughput.

### VRAM Limits

**vram_limit**: 45000000000 (45GB)

**Rationale**: Leave 5GB for system overhead, ensuring stable operation.

## Model Configuration

### Planning Model

**Model**: Llama 3.1 70B-Instruct-4bit
**Path**: `mlx-community/Llama-3.1-70B-Instruct-4bit`
**tensor_parallel**: 5
**batch_size**: 1
**quantization**: 4-bit
**temperature**: 0.7

**Rationale**:
- **Highest Quality**: 70B model for best reasoning and architectural design
- **Maximum GPU Usage**: All 5 GPUs, minimal batch for quality
- **Temperature 0.7**: More creative for architectural decisions

### Build Model

**Model**: DeepSeek Coder V2 16B-instruct
**Path**: `mlx-community/deepseek-coder-v2-lite-16b`
**tensor_parallel**: 2 (2 GPUs sufficient for coding tasks)
**batch_size**: 4 (faster iteration for small edits)
**quantization**: 4-bit
**temperature**: 0.3

**Rationale**:
- **Fast Iteration**: 16B model optimized for code generation
- **Balanced Performance**: 2 GPUs balance speed and memory
- **Batch Size 4**: Faster throughput for multiple small edits
- **Temperature 0.3**: More deterministic for code generation

### Review Model

**Model**: Llama 3.1 70B-Instruct-4bit
**Path**: `mlx-community/Llama-3.1-70B-Instruct-4bit`
**tensor_parallel**: 5
**batch_size**: 8 (fastest for quick analysis)
**quantization**: 4-bit
**temperature**: 0.5

**Rationale**:
- **Comprehensive Analysis**: 70B model for thorough review
- **Maximum GPU Usage**: All 5 GPUs
- **Batch Size 8**: Fastest for analysis tasks
- **Temperature 0.5**: Balanced for code review decisions

## Quantization Settings

**enabled**: true
**bits**: 4
**group_size**: 128

**Rationale**:
- **Memory Efficiency**: 4-bit quantization reduces memory usage 75%
- **Performance**: Minimal quality loss for code generation tasks
- **VRAM Optimization**: Fits 45GB VRAM constraint with 5x70B model

## Temperature Settings

**plan**: 0.7
**build**: 0.3
**review**: 0.5

**Rationale**:
- **Plan Mode**: More creative (0.7) for architecture decisions
- **Build Mode**: Balanced (0.3) for code generation
- **Review Mode**: Slightly higher (0.5) for balanced analysis

## Optimization Features

**Flash Attention**: Enabled
**KV Cache**: Enabled
**Enable Memory Efficient Attention**: Enabled

## Usage in OpenCode

### Environment Variables

```bash
# Set MLX environment variables before starting OpenCode
export MLX_TENSOR_PARALLEL=5
export MLX_MAX_GPUS=5
export MLX_VRAM_LIMIT=45000000000
export MLX_QUANTIZATION_BITS=4
export MLX_QUANTIZATION_GROUP_SIZE=128

export MLX_BATCH_PLAN=1
export MLX_BATCH_BUILD=4
export MLX_BATCH_REVIEW=8

export MLX_TEMP_PLAN=0.7
export MLX_TEMP_BUILD=0.3
export MLX_TEMP_REVIEW=0.5
```

### Model Paths

```yaml
# For use in .ai_rules localModels.mlx configuration
models:
  llama3.1_70b:
    path: "mlx-community/Llama-3.1-70B-Instruct-4bit"

  deepseek_coder_16b:
    path: "mlx-community/deepseek-coder-v2-lite-16b"
```

## Performance Characteristics

### Plan Model (Llama 3.1 70B - 5 GPUs, Batch 1)
- **Throughput**: High quality architectural decisions
- **Latency**: ~5-8 seconds for complex queries
- **VRAM Usage**: ~35GB (4-bit quantized)

### Build Model (DeepSeek 16B - 2 GPUs, Batch 4)
- **Throughput**: Fast code generation and edits
- **Latency**: ~2-3 seconds per response
- **VRAM Usage**: ~8GB per response (2 GPUs)
- **Token Efficiency**: Excellent for coding tasks

### Review Model (Llama 3.1 70B - 5 GPUs, Batch 8)
- **Throughput**: Fast analysis and code review
- **Latency**: ~3-5 seconds per analysis
- **VRAM Usage**: ~25GB (shared context for multiple files)
- **Accuracy**: High for code review and pattern recognition

## Recommendations

### For Maximum Performance

1. **Use Build Model** for implementation
   - DeepSeek 16B is faster and optimized for code generation
   - 2 GPUs with batch size 4 provides optimal throughput

2. **Use Review Model** for code analysis
   - Llama 3.1 70B with batch size 8 for fast analysis
   - All 5 GPUs for comprehensive review

3. **Keep MLX Background Process**
   - Pre-load models in memory for faster first token
   - Monitor GPU utilization with Activity Monitor

### For Quality Results

1. **Use Plan Model** for architectural decisions
   - Temperature 0.7, batch 1 for best quality

2. **Use Lower Temperature** for deterministic tasks
   - Build mode: 0.3 for code generation
   - Review mode: 0.5 for analysis

3. **Batch Multiple Small Operations**
   - Use batch size 4-8 for multiple edits
   - Reduces round-trips to MLX

---

**This configuration optimizes MLX for M2 Max hardware with 64GB RAM and 50GB VRAM.**
